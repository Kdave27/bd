{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d72361e-3b3d-4fa9-8220-9f46fd0bce7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "!wget https://raw.githubusercontent.com/neelamdoshi/Spark_neelam/main/diabetes.csv\n",
    "\n",
    "\n",
    "#read csv\n",
    "spark = SparkSession.builder.appName('deeplearn').getOrCreate()\n",
    "df = spark.read.csv('diabetes.csv', header=True, inferSchema=True)\n",
    "\n",
    "\n",
    "df.show()\n",
    "df.printSchema()\n",
    "\n",
    "# get the dimensions of the data\n",
    "(df.count() , len(df.columns))\n",
    "\n",
    "from pyspark.sql.functions import col, when\n",
    "numeric_cols = [col_name for col_name, dtype in my_data1.dtypes if dtype in ['int', 'double', 'float']]\n",
    "# Remove Outcome\n",
    "numeric_cols.remove('Outcome')\n",
    "\n",
    "for col_name in numeric_cols:\n",
    "    median = my_data1.fillna(0).approxQuantile(col_name, [0.5], 0.01)[0]\n",
    "    print(f\"Median of {col_name}: {median}\")\n",
    "    my_data1 = my_data1.withColumn(col_name, when(col(col_name) == 0, median).otherwise(col(col_name)))\n",
    "\n",
    "\n",
    "my_data1.show(5)\n",
    "\n",
    "\n",
    "my_data1.show()\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# specify the input and output columns of the vector assembler\n",
    "assembler = VectorAssembler(inputCols=['Pregnancies',\n",
    "                                       'Glucose',\n",
    "                                       'BloodPressure',\n",
    "                                       'SkinThickness',\n",
    "                                       'Insulin',\n",
    "                                       'BMI',\n",
    "                                       'DiabetesPedigreeFunction',\n",
    "                                       'Age'],\n",
    "                           outputCol='features')\n",
    "\n",
    "\n",
    "# transform the data\n",
    "final_data = assembler.transform(my_data1)\n",
    "\n",
    "# view the transformed vector\n",
    "\n",
    "final_data.select(\"features\",\"Outcome\").show(5)\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "xtrain, xtest = final_data.randomSplit([0.7, 0.3])\n",
    "\n",
    "\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'Outcome', maxIter=10000)\n",
    "\n",
    "lrModel = lr.fit(xtrain)\n",
    "\n",
    "predictions = lrModel.transform(xtest)\n",
    "\n",
    "predictions.show(5)\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator()\n",
    "evaluator.setLabelCol(\"Outcome\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cf843d-606e-4525-9463-d2c1d4a75567",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline\n",
    "!pip install pyspark\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "!wget https://raw.githubusercontent.com/neelamdoshi/Spark_neelam/main/diabetes.csv\n",
    "\n",
    "\n",
    "#read csv\n",
    "spark = SparkSession.builder.appName('deeplearn').getOrCreate()\n",
    "df = spark.read.csv('diabetes.csv', header=True, inferSchema=True)\n",
    "df.show()\n",
    "df.printSchema()\n",
    "\n",
    "def stage_1(x):\n",
    "    # Define numeric columns\n",
    "    numeric_cols = [col_name for col_name, dtype in x.dtypes\n",
    "                    if dtype in ['int', 'double', 'float']]\n",
    "\n",
    "    # Remove 'Outcome' column from numeric_cols if it exists\n",
    "    if 'Outcome' in numeric_cols:\n",
    "        numeric_cols.remove('Outcome')\n",
    "    print(\"Numeric Columns:\", numeric_cols)\n",
    "\n",
    "    # Impute zeros with median for numeric columns\n",
    "    for col_name in numeric_cols:\n",
    "        median = x.fillna(0).approxQuantile(col_name, [0.5], 0.01)[0]\n",
    "        x = x.withColumn(col_name, when(col(col_name) == 0, median).otherwise(col(col_name)))\n",
    "\n",
    "    return x, numeric_cols\n",
    "\n",
    "stage1, featuresCol = stage_1(df)\n",
    "stage1.show()\n",
    "\n",
    "# Vector Assembler stage\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "stage2 = VectorAssembler(inputCols=featuresCol,outputCol='features')\n",
    "\n",
    "# Logistic Regression stage\n",
    "stage3 = LogisticRegression(featuresCol='features', labelCol='Outcome', maxIter=100)\n",
    "\n",
    "\n",
    "#create pipeline\n",
    "regression_pipeline = Pipeline(stages=[stage2, stage3])\n",
    "\n",
    "\n",
    "# fit the pipeline for the trainind data\n",
    "model = regression_pipeline.fit(stage1)\n",
    "# transform the data\n",
    "model_train = model.transform(stage1)\n",
    "\n",
    "\n",
    "model_train.select('features', 'Outcome', 'prediction').show()\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator()\n",
    "evaluator.setLabelCol(\"Outcome\")\n",
    "evaluator.setPredictionCol(\"prediction\")\n",
    "accuracy = evaluator.evaluate(model_train)\n",
    "print(accuracy)\n",
    "\n",
    "#diff\n",
    "!pip install pyspark\n",
    "\n",
    "# Import necessary modules\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Imputer, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import pyspark.sql.types as tp\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DiabetesPredictionPipeline\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read the CSV file\n",
    "my_data = spark.read.csv('/content/diabetes.csv', header=True)\n",
    "\n",
    "# Define the schema for the data\n",
    "my_schema = tp.StructType([\n",
    "    tp.StructField(name='Pregnancies', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='Glucose', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='BloodPressure', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='SkinThickness', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='Insulin', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='BMI', dataType=tp.DoubleType(), nullable=True),\n",
    "    tp.StructField(name='DiabetesPedigreeFunction', dataType=tp.DoubleType(), nullable=True),\n",
    "    tp.StructField(name='Age', dataType=tp.IntegerType(), nullable=True),\n",
    "    tp.StructField(name='Outcome', dataType=tp.IntegerType(), nullable=True)\n",
    "])\n",
    "\n",
    "# Read the data again with the defined schema\n",
    "my_data = spark.read.csv('diabetes.csv', schema=my_schema, header=True)\n",
    "\n",
    "# Print the schema\n",
    "my_data.printSchema()\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "my_data = spark.read.csv('diabetes.csv', schema=my_schema, header=True)\n",
    "\n",
    "# Convert zeros to nulls\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "# List of columns where you want to replace zeros with null\n",
    "cols_to_check = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n",
    "\n",
    "# Replace zeros with nulls\n",
    "for col in cols_to_check:\n",
    "    my_data = my_data.withColumn(col, when(my_data[col] == 0, None).otherwise(my_data[col]))\n",
    "\n",
    "# Now you can proceed with the rest of your pipeline\n",
    "\n",
    "\n",
    "# Define stages for the pipeline\n",
    "imputer = Imputer(\n",
    "    inputCols=['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI'], # Specify columns to impute\n",
    "    outputCols=[\"{}_imputed\".format(c) for c in ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']]\n",
    ").setStrategy(\"median\")\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=['Pregnancies', 'Glucose_imputed', 'BloodPressure_imputed', 'SkinThickness_imputed',\n",
    "               'Insulin_imputed', 'BMI_imputed', 'DiabetesPedigreeFunction', 'Age'], # Use imputed columns\n",
    "    outputCol='features'\n",
    ")\n",
    "\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='Outcome', maxIter=10)\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(stages=[imputer, assembler, lr])\n",
    "\n",
    "# Split the data into training and test sets\n",
    "xtrain, xtest = my_data.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Fit the pipeline on training data\n",
    "pipeline_model = pipeline.fit(xtrain)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = pipeline_model.transform(xtest)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Outcome\", predictionCol=\"prediction\")\n",
    "accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68447428-5037-4677-ba53-b1d9b9e735c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CRUD\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Initialize a SparkSession\n",
    "spark = SparkSession.builder.appName(\"CRUD Operations\").getOrCreate()\n",
    "\n",
    "# Example data to simulate a table\n",
    "data = [\n",
    "    (1, \"Anil\", 21),\n",
    "    (2, \"Asad\", 23),\n",
    "    (3, \"Deep\", 22),\n",
    "    (4, \"Danish\", 25)\n",
    "]\n",
    "\n",
    "# Create schema\n",
    "columns = [\"id\", \"name\", \"age\"]\n",
    "\n",
    "# Create a DataFrame (This simulates the 'Create' operation for the table)\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# 1. CREATE - Show the initial table\n",
    "print(\"Initial Table:\")\n",
    "df.show()\n",
    "\n",
    "# 2. READ - Read from the table (Select specific columns)\n",
    "print(\"Read Operation (Select 'name' and 'age'):\")\n",
    "df.select(\"name\", \"age\").show()\n",
    "\n",
    "# 3. UPDATE - Let's update the 'age' of a person with 'id' = 2 (Bob)\n",
    "print(\"Update Aamir age to 32:\")\n",
    "df_update = df.withColumn(\"age\", when(col(\"id\") == 2, 28).otherwise(col(\"age\")))\n",
    "df_update.show()\n",
    "\n",
    "# 4. DELETE - Let's delete the row where the name is 'Danny'\n",
    "print(\"Delete operation (Remove Danish):\")\n",
    "df_delete = df_update.filter(col(\"name\") != \"Danish\")\n",
    "df_delete.show()\n",
    "\n",
    "# Stopping the Spark session\n",
    "spark.stop()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "                    .appName('SparkByExamples.com') \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "data = [(\"James\",\"Smith\",\"IND\",\"MH\"),(\"Michael\",\"Rose\",\"IND\",\"MP\"), \\\n",
    "    (\"Robert\",\"Williams\",\"IND\",\"UP\"),(\"Maria\",\"Jones\",\"IND\",\"TN\") \\\n",
    "  ]\n",
    "columns=[\"firstname\",\"lastname\",\"country\",\"state\"]\n",
    "df=spark.createDataFrame(data=data,schema=columns)\n",
    "df.show()\n",
    "print(df.collect())\n",
    "\n",
    "\n",
    "states1=df.rdd.map(lambda x: x[3]).collect()\n",
    "print(states1)\n",
    "from collections import OrderedDict\n",
    "res = list(OrderedDict.fromkeys(states1))\n",
    "print(res)\n",
    "\n",
    "\n",
    "\n",
    "#Example 2\n",
    "states2=df.rdd.map(lambda x: x.state).collect()\n",
    "print(states2)\n",
    "\n",
    "states3=df.select(df.state).collect()\n",
    "print(states3)\n",
    "\n",
    "states4=df.select(df.state).rdd.flatMap(lambda x: x).collect()\n",
    "print(states4)\n",
    "\n",
    "states5=df.select(df.state).toPandas()['state']\n",
    "states6=list(states5)\n",
    "print(states6)\n",
    "\n",
    "pandDF=df.select(df.state,df.firstname).toPandas()\n",
    "print(list(pandDF['state']))\n",
    "print(list(pandDF['firstname']))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
